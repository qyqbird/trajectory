# 调参文档
https://c740uhpr2i.feishu.cn/sheets/B7CFs8ijih1c9xtqKLAcL6FdnVg?sheet=pasm3e
参数很重要

#Qwen2-7B-instruct-awq
1. 作品不能带书名号,F1 50->57
2. output 与schema 完全对齐，删除无关输出,JSON异常只有3 56->59

3. 有几个品类的作品也需要进行整理《》；二层字典的key。 59->61.4
4. 个性化source example： 61.4->60; 应该是有些prompt不适合,或者更简单了,
    法律：有很多模糊地方（比如实施日期，颁发日期），不知道如何抽取
	很有意思：即使只保留：default,区域，场所，词汇，饮食, 61.4 -> 61.0
	delete 词汇demo 61->61.2
	reback


# UniNER-7B-all：26G  似乎应该用:UniNER-7B-definition  对于示意改写更兼容
1. Llama + ChatGPT生成的数据 -> 英文友好，中文估计支持不行; 耗时从2min-》12min左右；还能run起来
如果全面微调中文，成本太高。 
2. Instruction 和预训练不一致。
需要先生成实体，然后 实体+属性，生成每一个属性值 才比较适配
先delay

# Qwen2-72B-awq
1. 慢慢下载; 分片段 GIT_LFS_SKIP_SMUDGE=1 git clone https://www.modelscope.cn/qwen/Qwen2-72B-Instruct-AWQ.git  直接git clone : 一直不成功,被killed
2. git lfs pull --include=model-00004-of-00011.safetensors

同样的代码:显卡OOM

# OneKE探索

# SFT
1. 数据,以哪个作为基准呢?
	 多个大模型结果进行比对,一致可以做为标准
	以初赛数据作为参考
2. 仔细查看数据:shcema 似乎有不合理的地方,与input不配对,这种情况问题很大
