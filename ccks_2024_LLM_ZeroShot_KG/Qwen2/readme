基于Qwen2官方的微调配置
1. 把IEpile 数据转换为chatml 格式
2. RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::Half != c10::BFloat16
	删除配置:--bf16 True \  
3. ValueError: auto-awq kernels is needed to be installed to use `.backward()`. Make sure to install the auto-awq kernels by following the installation guides in https://github.com/casper-hansen/AutoAWQ_kernels
Cuda 11.8: uname -m: x86 ,因此选择安装:pip install https://github.com/casper-hansen/AutoAWQ_kernels/releases/download/v0.0.7/autoawq_kernels-0.0.7+cu118-cp310-cp310-linux_x86_64.whl --no-dependencies
4. 调参,看时间:5000样本1h 12min  2Epoch

5. 推理vllm 加载：ValueError: LoRA rank 64 is greater than max_lora_rank 16.
设置参数 max_lora_rank=64 即可

性能下滑：61.2 -> 60.3 符合初步的数据; 尝试取消example中的无，-> 59


调整微调参数
1. Epoch 1; Lora 后，耗时5min  学习率 -> 61
2. 一些不支持设置的参数：--lora_alpha


思路还是明确的，在Qwen2-7B上做实验


lora微调：也有这种指明参数的加载方式
config =LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],
    inference_mode=False,# 训练模式
    r=8,# Lora 秩
    lora_alpha=32,# Lora alaph，具体作用参见 Lora 原理
    lora_dropout=0.1,# Dropout 比例
)

model = get_peft_model(model, config)

lora.r = 32; alpha = 64
数据比例上做了均匀：RE,EE,NER; 样本5000-> 6000; 61->60.8


# InstrucIE 数据生成
schema中”产品-作品“