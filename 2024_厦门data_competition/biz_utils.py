#coding:utf-8
import re
import os
import torch
import logging
import copy
import pandas as pd
logger = logging.getLogger(__name__)

FILTER = ['ç«å“æ‰‹æœº','ç«å“ç”µè„‘','ç«å“ç”Ÿæ€é“¾','ç«å“æ‰‹ç¯','ç«å“æ‰‹è¡¨','ç«å“è€³æœº','ç«å“ç”µè§†','ç«å“é«˜ç®¡','ç«å“å“ç‰Œ']
def bert_tokenize_content(lines, tokenizer):
	tokens_line = []
	for line in lines:
		tokens = tokenizer.tokenize(line)
		result = []
		for token in tokens:
			if token.startswith('##'):
				token = token[2:]
			elif token == '[UNK]':
				token = ' '
			result.append(token)
		tokens_line.append(''.join(result))
	return tokens_line

def get_label2id_mapping(label_file):
	"""
	è·å–ç¬”ç”µæ ‡ç­¾ä½“ç³»
	:param label_file:
	:return:
	"""
	df_in = pd.read_csv(label_file).fillna("")
	category_label2id_table = {}
	for index, row in df_in.iterrows():
		category = row['cate_name'].strip()
		# ç¬”è®°æœ¬ä½¿ç”¨äºŒçº§æ ‡ç­¾,å…¶å®ƒå“ç±»ä½¿ç”¨ä¸‰çº§æ ‡ç­¾
		third = str(row["failure_third_name"]).strip()
		second = str(row["failure_second_name"]).strip()
		first = str(row["failure_first_name"]).strip()
		third_id = str(row["failure_third_id"]).strip()
		second_id = str(row["failure_second_id"]).strip()
		first_id = str(row["failure_first_id"]).strip()

		target_dict = {}
		if category in category_label2id_table:
			target_dict = category_label2id_table[category]

		if category == "ç¬”è®°æœ¬":
			target_dict[second] = [first, first_id, second, second_id, third, third_id]
		else:
			target_dict[third] = [first, first_id, second, second_id, third, third_id]
		category_label2id_table[category] = target_dict
	return category_label2id_table

def get_bizlabel_table(filename):
	dataframe = pd.read_excel(filename, sheet_name=None)	#æŒ‡å®šä¸ºNoneï¼Œè¯»å–æ‰€æœ‰çš„sheeté¡µé¢ï¼Œå¦åˆ™åªè¯»å–ç¬¬ä¸€ä¸ªsheeté¡µé¢,è¿”å›æ˜¯ä¸ä¸€æ ·çš„
	biz_label_config = {}

	for sheet_name, sheet in dataframe.items():
		aspects_opinions_label = {}	#{phone: {aspect:{opinion: value}}, "notebook":{}}
		merge_entity = set()
		for index, row in sheet.iterrows():
			aspects = row["aspect"]
			opinions = row["opinions"]
			label = str(row["label"]).strip()

			if not pd.isna(aspects)  and not pd.isna(opinions):
				aspects = str(aspects).strip().split(",")
				opinions = str(opinions).strip().split(",")
				for aspect in aspects:
					if aspect in aspects_opinions_label:
						for opinion in opinions:
							aspects_opinions_label[aspect][opinion] = label
					else:
						tmp = {}
						for opinion in opinions:
							tmp[opinion] = label
						aspects_opinions_label[aspect] = tmp
			elif not pd.isna(opinions):
				opinions = str(opinions).strip().split(",")
				if len(opinions) > 0:
					for opinion in opinions:
						tmp = {}
						tmp[opinion] = label
						aspects_opinions_label[opinion] = tmp
			elif label == "merge_key":
				merge_keys = str(aspects).strip().split(",")
				if len(merge_keys) > 0:
					merge_entity.update(merge_keys)
		print(f"{sheet_name}\tlabels:{len(aspects_opinions_label)}\tmerge_key:{len(merge_entity)}")
		print(f"{aspects_opinions_label}\nmerge_key:{merge_entity}")
		biz_label_config[sheet_name] = [aspects_opinions_label, merge_entity]

	return biz_label_config

def get_bizlabel_table(filename):
	dataframe = pd.read_excel(filename, sheet_name=None)	#æŒ‡å®šä¸ºNoneï¼Œè¯»å–æ‰€æœ‰çš„sheeté¡µé¢ï¼Œå¦åˆ™åªè¯»å–ç¬¬ä¸€ä¸ªsheeté¡µé¢,è¿”å›æ˜¯ä¸ä¸€æ ·çš„
	biz_label_config = {}

	for sheet_name, sheet in dataframe.items():
		aspects_opinions_label = {}	#{phone: {aspect:{opinion: value}}, "notebook":{}}
		merge_entity = set()
		for index, row in sheet.iterrows():
			aspects = row["aspect"]
			opinions = row["opinions"]
			label = str(row["label"]).strip()

			if not pd.isna(aspects)  and not pd.isna(opinions):
				aspects = str(aspects).strip().split(",")
				opinions = str(opinions).strip().split(",")
				for aspect in aspects:
					if aspect in aspects_opinions_label:
						for opinion in opinions:
							aspects_opinions_label[aspect][opinion] = label
					else:
						tmp = {}
						for opinion in opinions:
							tmp[opinion] = label
						aspects_opinions_label[aspect] = tmp
			elif not pd.isna(opinions):
				opinions = str(opinions).strip().split(",")
				if len(opinions) > 0:
					for opinion in opinions:
						tmp = {}
						tmp[opinion] = label
						aspects_opinions_label[opinion] = tmp
			elif label == "merge_key":
				merge_keys = str(aspects).strip().split(",")
				if len(merge_keys) > 0:
					merge_entity.update(merge_keys)
		print(f"{sheet_name}\tlabels:{len(aspects_opinions_label)}\tmerge_key:{len(merge_entity)}")
		print(f"{aspects_opinions_label}\nmerge_key:{merge_entity}")
		biz_label_config[sheet_name] = [aspects_opinions_label, merge_entity]
	return biz_label_config

#
# biz_label_config = get_bizlabel_table('aspect_fault_config.xlsx')
# print(biz_label_config)

'''
åŸºæœ¬è¦ç”¨,ç©ºæ ¼åˆ‡å‰²å¥å­äº†
'''
def detail_cut_sentence(content):
	result = []
	if len(content) == 0:
		return result
	if len(content) <= 78:
		result.append(content)
	else:
		bak = re.split("(ï¼Œ|,| )", content)
		ss_con = ""
		for ydx in range(0, len(bak), 2):
			sub = bak[ydx]
			if ydx + 1 < len(bak):
				sub += bak[ydx+1]
			ss_con += sub
			if len(ss_con) >= 66:
				result.append(ss_con)
				ss_con = ""
		if len(ss_con) > 0:
			result.append(ss_con)
	return result

#ç²—ç²’åº¦åˆ‡å¥å­
def pre_cut_sentence(content):
	items = re.split("(ã€‚|ï¼Ÿ|ï¼|ï¼›)", content.strip())
	result = []
	stringbuilder = ""
	for idx in range(0, len(items), 2):
		item = items[idx].strip()
		comma = ''
		if item == "":
			continue
		if idx+1 < len(items):
			comma = items[idx+1]

		bak = stringbuilder
		stringbuilder = stringbuilder + item + comma
		length = len(stringbuilder)
		if length < 60:
			continue
		elif length >= 60 and length <= 80:
			result.append(stringbuilder)
			stringbuilder = ""
		elif length >= 80:
			if len(bak) > 0:
				result.extend(detail_cut_sentence(bak))
			stringbuilder = item + comma
	if len(stringbuilder) > 0:
		result.extend(detail_cut_sentence(stringbuilder))
	result = [x.strip() for x in result]
	return result

url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
def clear_content(line):
	line = line.strip().lower().replace("â€", '"').replace('â€œ', '"').replace('â•', '+').\
		replace('â€”', '-').replace('â€™',"'").replace('â€¦', '...')
	line = line.replace('[doge]','')
	line = line.replace('[èˆ”å±]','')
	line = line.replace('[è€å…‹å˜´]','')
	line = line.replace('[æ‰“call]','')
	line = line.replace('\u200b','')	#<200b> 0å®½æ–­è¨€ç‰¹æ®Šå­—ç¬¦
	line = line.replace('[é»‘çº¿]','')
	line = line.replace('[å¹¶ä¸ç®€å•]','')
	line = line.replace('[è£‚å¼€]','')
	line = line.replace('[å¤±æœ›]','')
	line = line.replace('ğŸ’©','å±')
	line = line.replace('[è¡°]','').replace("[å›¾ç‰‡]", "").replace('image_emoticon', "")
	line = ' '.join(line.split())	#æŠŠå¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸º1ä¸ªï¼Œé¿å…åˆ†è¯å‡ºç°å°‘ç©ºæ ¼å¯¼è‡´çš„å¼‚å¸¸
	urls = url_pattern.findall(line)
	for url in urls:
		line = line.replace(url, 'url')

	if line.startswith('"'):
		line = line[1:]
	if line.endswith('"'):
		line = line[:-1]
	#è½¬å‘æ•°æ®
	if re.search('å›å¤@.*:', line):
		line = re.sub('å›å¤@.*?:','', line)
	if re.search('@.* ', line):
		line = re.sub('@.*? ','', line)
	if re.search('@.*:', line):
		line = re.sub('@.*:','', line)

	if line.startswith("å›å¤"):
		line = line[2:]
	if 'Â¡è¯„è®ºé…å›¾' in line:
		line = line.replace('Â¡è¯„è®ºé…å›¾', '')

	line = line.strip().lower()
	if len(line) == 0:
		return []
	else:
		items = pre_cut_sentence(line)
		return items

def key_fault_insert(content, fault, extract_shouhou_map,map_info):
	if fault['label'] in extract_shouhou_map:
		target_label = extract_shouhou_map[fault['label']][fault['label']]
		target_label = label_rule_check(content, target_label, None, fault)
		map_info.append(target_label)
		return True
	return False

def entity_fault_insert(content, aspect, fault, extract_shouhou_map, map_info):
	if aspect['label'] in extract_shouhou_map and fault['label'] in extract_shouhou_map[aspect['label']]:
		target = label_rule_check(content, extract_shouhou_map[aspect['label']][fault['label']], aspect, fault)
		map_info.append(target)
		return 1
	else:
		return 0

def aspect_opinion_insert_delete(content, aspect, fault, extract_shouhou_map, map_info):
	if aspect['label'] in extract_shouhou_map and fault['label'] in extract_shouhou_map[aspect['label']]:
		target = label_rule_check(content, extract_shouhou_map[aspect['label']][fault['label']], aspect, fault)
		map_info.append(target)
	elif fault['label'] in extract_shouhou_map:
		map_info.append(extract_shouhou_map[fault['label']][fault['label']])

ZH_PUNCITON = re.compile(r"ã€‚|ï¼Ÿ|ï¼")

# TODO: abc/ab æŠŠæ ‡ç­¾æ”¾ä¸­é—´ï¼Œç„¶åè¿‡æ»¤
def aspect_faults_match(content, line, extract_shouhou_map):
	map_info = []
	print(f"before abc:{content}\t{line}\t{map_info}")
	line = abc_pattern_merge(content, extract_shouhou_map, line, map_info)
	ab_pattern_merge(content, extract_shouhou_map, line, map_info)
	need_match_aspect = []
	need_match_opinion = None
	print(f"after abc:{content}\t{line}\t{map_info}")
	#2. æ— äº¤é›†éƒ¨åˆ†çº¿æ€§æœç´¢
	for item in line:
		if item['type'] == 'aspect':
			# print(f"{need_match_aspect}\t{need_match_opinion}\t{item}\t{map_info}")
			need_match_aspect.append(item)
			if need_match_opinion is not None:
				tmp_content = content[need_match_opinion['end']:item['start']]
				flag = -1
				if 'ã€‚' not in tmp_content:
					if ',' in tmp_content or 'ï¼Œ' in tmp_content or ' ' in tmp_content:
						if len(tmp_content) < 10:
							flag = entity_fault_insert(content, need_match_aspect[-1], need_match_opinion, extract_shouhou_map, map_info)
					elif len(tmp_content) < 16:
							flag = entity_fault_insert(content, need_match_aspect[-1], need_match_opinion, extract_shouhou_map, map_info)
				if flag > 0:
					need_match_aspect = []
					need_match_opinion = None
				elif flag <= 0:
					if flag == 0:
						logger.info(f"match error:{content}\t{item}\t{need_match_opinion}")
					key_fault_insert(content, need_match_opinion, extract_shouhou_map, map_info)
					need_match_opinion = None
		else:
			# print(f"{need_match_aspect}\t{need_match_opinion}\t{item}\t{map_info}\t{line}")
			if len(need_match_aspect) == 0:
				if need_match_opinion is not None:
					key_fault_insert(content, need_match_opinion, extract_shouhou_map, map_info)
				need_match_opinion = item
			else:
				tmp_content = content[need_match_aspect[-1]['end']:item['start']]
				if len(ZH_PUNCITON.findall(tmp_content)) > 1 or (('ï¼›' in tmp_content or 'ï¼Œ' in tmp_content) and len(tmp_content) > 13) or len(tmp_content) > 21:
					#ä¸»è¯­è§‚ç‚¹æœ‰è·ç¦»
					need_match_aspect = []
					need_match_opinion = item
					continue

				if need_match_aspect[-1]['label'] in extract_shouhou_map and item['label'] in extract_shouhou_map[need_match_aspect[-1]['label']]:
					# æ‰¬å£°å™¨å’Œå¬ç­’å¤±æ•ˆ  è§£å†³æ‰¬å£°å™¨çš„é—®é¢˜
					if len(need_match_aspect) >= 2 and (need_match_aspect[-1]['start'] - need_match_aspect[-2]['end']) <= 1:
						if need_match_aspect[-2]['label'] in extract_shouhou_map and item['label'] in extract_shouhou_map[need_match_aspect[-2]['label']] and 'æ‰‹æœº' not in need_match_aspect[-2]['label']:
							insert = extract_shouhou_map[need_match_aspect[-2]['label']][item['label']]
							map_info.append(insert)
					target_label = label_rule_check(content, extract_shouhou_map[need_match_aspect[-1]['label']][item['label']], need_match_aspect[-1], item)
					map_info.append(target_label)
				elif item['label'] in extract_shouhou_map:
					target_label = extract_shouhou_map[item['label']][item['label']]
					target_label = label_rule_check(content, target_label, None, item)
					map_info.append(target_label)
				need_match_aspect = []

	if need_match_opinion != None:
		if need_match_opinion['label'] in extract_shouhou_map:
			target_label = extract_shouhou_map[need_match_opinion['label']][need_match_opinion['label']]
			target_label = label_rule_check(content, target_label, None, need_match_opinion)
			map_info.append(target_label)
	map_info = filter(lambda x: len(x) > 0, map_info)
	map_info = list(set(map_info))
	return map_info


def abc_pattern_merge(content, extract_shouhou_map, line, map_info):
	cursor = 0
	while cursor + 2 < len(line):
		left, middle, right = line[cursor], line[cursor+1], line[cursor+2]
		if left['type'] == 'aspect' or middle['type'] == 'aspect' or right['type'] == 'aspect':
			if left['end'] >= middle['start'] and middle['end'] >= right['start']:
				if left['type'] == "opinion" and middle['type'] == "aspect" and right['type'] == "opinion":
					aspect_opinion_insert_delete(content, middle, left, extract_shouhou_map, map_info)
					aspect_opinion_insert_delete(content, middle, right, extract_shouhou_map, map_info)
					line.remove(left)
					line.remove(middle)
					line.remove(right)
				elif left['type'] == "aspect" and middle['type'] == "aspect" and right['type'] == "opinion":
					if left['label'] not in ["æ‰‹æœº","å°ç±³æ‰‹æœº","ç«å“æ‰‹æœº"]:
						aspect_opinion_insert_delete(content, left, right, extract_shouhou_map, map_info)
					aspect_opinion_insert_delete(content, middle, right, extract_shouhou_map, map_info)
					line.remove(left)
					line.remove(middle)
					line.remove(right)
				elif left['type'] == "aspect" and middle['type'] == "opinion" and right['type'] == "opinion":
					aspect_opinion_insert_delete(content, left, middle, extract_shouhou_map, map_info)
					aspect_opinion_insert_delete(content, left, right, extract_shouhou_map, map_info)
					line.remove(left)
					line.remove(middle)
					line.remove(right)
				elif left['type'] == "opinion" and middle['type'] == "aspect" and right['type'] == "aspect":
					aspect_opinion_insert_delete(content, middle, left, extract_shouhou_map, map_info)
					aspect_opinion_insert_delete(content, right, left, extract_shouhou_map, map_info)
					line.remove(left)
					line.remove(middle)
					line.remove(right)
		cursor += 1
	return line

# aspect-opinionæœ‰äº¤é›†ï¼Œåˆ™ç›´æ¥merge ä¸ºlabel
def ab_pattern_merge(content, extract_shouhou_map, line, map_info):
	cursor = 0
	while cursor + 1 < len(line):
		start_item = line[cursor]
		end_item = line[cursor+1]
		if start_item['type'] == 'aspect' or end_item['type'] == 'aspect':
			if end_item['start'] <= start_item['end']:
				if start_item['type'] == "aspect" and end_item['type'] == "opinion":
					aspect_opinion_insert_delete(content, start_item, end_item, extract_shouhou_map, map_info)
					line.remove(start_item)
					line.remove(end_item)
				elif end_item['type'] == "aspect" and start_item['type'] == "opinion":
					aspect_opinion_insert_delete(content, end_item, start_item, extract_shouhou_map, map_info)
					line.remove(start_item)
					line.remove(end_item)
		cursor += 1
	return line

def post_rule_notebook(content, map_info):
	return map_info

def post_rule_pad(content, map_info):
	if 'å¾…æœº' in content:
		if 'åŠŸè€—ä¸æ»¡æ„--ä¸ƒå¤©æ— ç†ç”±' in map_info:
			map_info.remove('åŠŸè€—ä¸æ»¡æ„--ä¸ƒå¤©æ— ç†ç”±')
			map_info.append('å¾…æœºæ—¶è€—ç”µå¼‚å¸¸')
		elif 'å‘çƒ­ä¸æ»¡æ„--ä¸ƒå¤©æ— ç†ç”±' in map_info:
			map_info.remove('å‘çƒ­ä¸æ»¡æ„--ä¸ƒå¤©æ— ç†ç”±')
			map_info.append('å¾…æœºæ—¶æ‰‹æœºå‘çƒ«/é«˜æ¸©/å‘çƒ­')

	if 'å‰ç½®' in content:
		map_info = [item.replace('åç½®', 'å‰ç½®') for item in map_info]

	return map_info

def post_rule_phone(content, map_info):
	# åç½®å¤„ç†è§„åˆ™
	if 'ä¸»å±é»‘å±(æœ‰å£°éŸ³/éœ‡åŠ¨/å¯æ‰“è¿›ç”µè¯)' in map_info:
		if 'æ­»æœº' in content or 'é»‘å±å…³æœº' in content:
			map_info.remove("ä¸»å±é»‘å±(æœ‰å£°éŸ³/éœ‡åŠ¨/å¯æ‰“è¿›ç”µè¯)")
	return map_info

def post_rule_bracket(content, map_info):
	# åç½®å¤„ç†è§„åˆ™
	if 'é»‘å±å…³æœº' in content and 'ä¸»å±é»‘å±(æœ‰å£°éŸ³/éœ‡åŠ¨/å¯æ‰“è¿›ç”µè¯)' in map_info:
		map_info.remove("ä¸»å±é»‘å±(æœ‰å£°éŸ³/éœ‡åŠ¨/å¯æ‰“è¿›ç”µè¯)")
	return map_info

def label_rule_check(content, biz_label, aspect, opinion):
	if aspect is None:
		start = max(0, opinion['start'] - 5)
		end = min(len(content), opinion['end'] + 5)
	else:
		start = aspect['start'] if aspect['start'] < opinion['start'] else opinion['start']
		end = aspect['end'] if aspect['end'] > opinion['end'] else opinion['end']
		start = max(0, start - 5)
		end = min(len(content), end + 5)
	search_content = content[start:end]
	# åç½®å¤„ç†è§„åˆ™
	if 'å‰ç½®' in search_content or 'å‰æ‘„' in content:
		biz_label = biz_label.replace('åç½®', 'å‰ç½®', 1).replace('åæ‘„', 'å‰æ‘„', 1)
	elif 'è“ç‰™è€³æœº' in search_content:
		biz_label = biz_label.replace('è€³æœºå£°éŸ³ç›¸å…³æ•…éšœ', 'è“ç‰™å£°éŸ³ç›¸å…³æ•…éšœ', 1)

	if 'é€šè¯' in search_content and 'éé€šè¯' not in content:
		biz_label = biz_label.replace('éé€šè¯çŠ¶æ€éº¦å…‹æ— éŸ³', 'é€šè¯æ—¶éº¦å…‹é£æ— å£°', 1)
		biz_label = biz_label.replace('éé€šè¯çŠ¶æ€å¬ç­’æ— éŸ³', 'é€šè¯æ—¶å¬ç­’æ— å£°', 1)
		biz_label = biz_label.replace('éé€šè¯çŠ¶æ€å¬ç­’æ‚éŸ³', 'é€šè¯æ—¶å¬ç­’æ‚éŸ³', 1)
		biz_label = biz_label.replace('éé€šè¯çŠ¶æ€å¬ç­’å£°å°', 'é€šè¯æ—¶å¬ç­’å£°éŸ³å°', 1)

	# å€¾å‘äºæ‰‹æœºä¸å……ç”µ
	if 'æ— çº¿å……' in search_content:
		biz_label = biz_label.replace("æ‰‹æœºä¸å……ç”µ", "æ‰‹æœºæ— çº¿å……ç”µä¸å……ç”µ", 1)
		biz_label = biz_label.replace("æ‰‹æœºå……ç”µæ…¢", "æ‰‹æœºæ— çº¿å……ç”µæ…¢", 1)
		biz_label = biz_label.replace("å……ç”µæ—¶æ‰‹æœºå‘çƒ«/é«˜æ¸©/å‘çƒ­", "æ‰‹æœºæ— çº¿å……ç”µæ—¶å‘çƒ­", 1)
	if 'å¾…æœº' in search_content:
		biz_label = biz_label.replace("ä½¿ç”¨å…¶ä»–åº”ç”¨æ—¶è€—ç”µå¿«/ç»­èˆªå·®", "å¾…æœºæ—¶è€—ç”µå¼‚å¸¸", 1)
		biz_label = biz_label.replace("åŠŸè€—ä¸æ»¡æ„--ä¸ƒå¤©æ— ç†ç”±", "å¾…æœºæ—¶è€—ç”µå¼‚å¸¸", 1)
		biz_label = biz_label.replace("æ‰‹æœºå……ç”µæ…¢", "å¾…æœºæ—¶æ‰‹æœºå‘çƒ«/é«˜æ¸©/å‘çƒ­", 1)
		biz_label = biz_label.replace("å……ç”µæ—¶æ‰‹æœºå‘çƒ«/é«˜æ¸©/å‘çƒ­", "å¾…æœº/ç»­èˆª/è€—ç”µæ•…éšœå…¶ä»–", 1)

	if 'æ— ä¿¡å·' in content or 'æ²¡æœ‰ä¿¡å·' in content or 'æ²¡ä¿¡å·' in content:
		biz_label = biz_label.replace("ä¿¡å·å·®/æ ¼æ•°å°‘ï¼ˆ2G/3G/4G/5Gï¼‰", "æ— ä¿¡å·æˆ–æ— æœåŠ¡", 1)

	if 'å†…å±' in search_content:
		biz_label = biz_label.replace("ä¸»å±å¤–ç»ç’ƒç ´æŸ/ç¢è£‚", "ä¸»å±å†…å±æŸä¼¤ï¼ˆå¤–å±æ— æŸä¼¤ï¼‰", 1)
		biz_label = biz_label.replace("ä¸»å±æ˜¾ç¤ºæ•…éšœå…¶ä»–", "ä¸»å±å†…å±æŸä¼¤ï¼ˆå¤–å±æ— æŸä¼¤ï¼‰", 1)
		biz_label = biz_label.replace("ä¸»å±è§¦æ‘¸å±åˆ’ä¼¤", "ä¸»å±å†…å±æŸä¼¤ï¼ˆå¤–å±æ— æŸä¼¤ï¼‰", 1)
		biz_label = biz_label.replace("å‰¯å±å±å¹•ç ´æŸ/ç¢è£‚", "å‰¯å±å±å¹•ç ´æŸï¼ˆå†…å±ï¼‰", 1)
		biz_label = biz_label.replace("å‰¯å±è§¦æ‘¸å±åˆ’ä¼¤", "å‰¯å±å±å¹•ç ´æŸï¼ˆå†…å±ï¼‰", 1)
	if 'ä¸è·Ÿæ‰‹' in search_content or 'è¯¯è§¦' in search_content:
		biz_label = biz_label.replace("ä¸»å±è§¦æ‘¸å±å±€éƒ¨å¤±çµ", "ä¸»å±å±å¹•è¾¹ç¼˜è¯¯è§¦")
		biz_label = biz_label.replace("å‰¯å±ï¼ˆå°å±ï¼‰è§¦æ‘¸å±å…¨å±å¤±çµ", "å‰¯å±ï¼ˆå°å±ï¼‰å±å¹•è¾¹ç¼˜è¯¯è§¦")

	return biz_label


RULE_DICT = {"phone": post_rule_phone, "laptop": post_rule_notebook, "pad": post_rule_pad}
FILTER_DICT = {"phone": ['ç”µè„‘','æ‰‹ç¯','å¹³æ¿','è§¦æ§ç¬”','ç¡çœ ','å¿ƒç‡','è®¡æ­¥','è¡€æ°§','å……ç”µç›’','è¡¨ç›˜','è¡¨å¸¦','èºä¸','office','ç¡¬ç›˜','æ˜¾å¡','å®¢æœ','å®‰è£…','å”®å','åŒ…è£…','ç‰©æµ','å‘è´§','æœºé¡¶ç›’','hdmi'],
			   "laptop": ['æ‰‹ç¯','å¹³æ¿','è§¦æ§ç¬”','ç¡çœ ','å¿ƒç‡','è®¡æ­¥','è¡€æ°§','å……ç”µç›’','è¡¨ç›˜','è¡¨å¸¦','èºä¸','å®¢æœ','å®‰è£…','å”®å','åŒ…è£…','ç‰©æµ','å‘è´§','æœºé¡¶ç›’','æ‰‹ç”µç­’'],
			    "pad": ['ç”µè„‘','æ‰‹ç¯','ç¡çœ ','å¿ƒç‡','è®¡æ­¥','è¡€æ°§','å……ç”µç›’','è¡¨ç›˜','è¡¨å¸¦','èºä¸','ç¡¬ç›˜','æ˜¾å¡','å®¢æœ','å®‰è£…','å”®å','åŒ…è£…','ç‰©æµ','å‘è´§','æœºé¡¶ç›’','hdmi']}
def generate_bizlabelID(map_info, bizid_mapping):
	fir = list(map(lambda item: bizid_mapping[item][0] if item in bizid_mapping else None, map_info))
	fir_id = list(map(lambda item: bizid_mapping[item][1] if item in bizid_mapping else None, map_info))
	sec = list(map(lambda item: bizid_mapping[item][2] if item in bizid_mapping else None, map_info))
	sec_id = list(map(lambda item: bizid_mapping[item][3] if item in bizid_mapping else None, map_info))
	third = list(map(lambda item: bizid_mapping[item][4] if item in bizid_mapping else None, map_info))
	third_id = list(map(lambda item: bizid_mapping[item][5] if item in bizid_mapping else None, map_info))
	fir = list(filter(lambda x: x != None, fir))
	fir_id = list(filter(lambda x: x != None, fir_id))
	sec = list(filter(lambda x: x != None, sec))
	sec_id = list(filter(lambda x: x != None, sec_id))
	third = list(filter(lambda x: x != None, third))
	third_id = list(filter(lambda x: x != None, third_id))
	return fir, fir_id, sec, sec_id, third, third_id


def get_args(proj_name, biz_name, model_dir):
    return torch.load(os.path.join(proj_name, biz_name, model_dir, 'training_args.bin'))


